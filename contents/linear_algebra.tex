\section{Linear Algebra}
\label{sec:linear_algebra}
% PROMPT: Summarize key linear algebra concepts needed for LLMs

\noindent
\textbf{Linear algebra} underpins many of the mathematical operations central to neural networks and, by extension, Large Language Models (LLMs). From the representation of word embeddings as vectors to the matrix multiplication performed in attention mechanisms, a solid understanding of linear algebra is indispensable. This section revisits the core concepts you will need.

\subsection{Vector Spaces, Norms, and Inner Products}
\noindent
A \textbf{vector space} over a field (commonly the real numbers $\mathbb{R}$) is a set of elements called \emph{vectors} that can be added and scaled by real numbers according to certain axioms. In the context of LLMs:
\begin{itemize}
    \item \textbf{Word Embeddings as Vectors.} Each token (word or subword) in a vocabulary can be represented as a vector in $\mathbb{R}^d$, where $d$ is the embedding dimension. 
    \item \textbf{Inner Product (Dot Product).} For two vectors $\mathbf{u}, \mathbf{v} \in \mathbb{R}^d$, the inner product is $\mathbf{u} \cdot \mathbf{v} = \sum_{i=1}^{d} u_i v_i$. This measure of similarity underlies key computations in attention mechanisms.
    \item \textbf{Norms.} The Euclidean norm (or $L_2$ norm) is $\|\mathbf{v}\|_2 = \sqrt{\sum_{i=1}^{d} v_i^2}$. Norms help quantify vector magnitude, which is crucial for regularization and gradient scaling in large models.
\end{itemize}

\subsection{Matrices: Multiplication, Inversion, Eigenvalues/Eigenvectors}
\noindent
A \textbf{matrix} $\mathbf{M} \in \mathbb{R}^{m \times n}$ is an arrangement of real numbers into $m$ rows and $n$ columns. Matrices represent linear transformations in high-dimensional spaces, which are fundamental to how neural networks process data.

\begin{itemize}
    \item \textbf{Matrix Multiplication.} 
    For $\mathbf{A} \in \mathbb{R}^{m \times n}$ and $\mathbf{B} \in \mathbb{R}^{n \times p}$, their product $\mathbf{C} = \mathbf{A}\mathbf{B}$ has dimensions $m \times p$. Each element $c_{ij}$ is the dot product of the $i$th row of $\mathbf{A}$ with the $j$th column of $\mathbf{B}$. This operation is central to every feedforward and attention layer in a Transformer.

    \item \textbf{Matrix Inversion.}
    An \textbf{invertible} (or nonsingular) square matrix $\mathbf{M} \in \mathbb{R}^{n \times n}$ has an inverse $\mathbf{M}^{-1}$ such that $\mathbf{M}\mathbf{M}^{-1} = \mathbf{I}$, where $\mathbf{I}$ is the identity matrix. Although direct matrix inversion is not frequently performed in LLMs (due to high dimensionality and potential non-invertibility), understanding the concept helps in analyzing linear transformations and stability.

    \item \textbf{Eigenvalues and Eigenvectors.}
    For a square matrix $\mathbf{A} \in \mathbb{R}^{n \times n}$, a vector $\mathbf{v} \neq \mathbf{0}$ is an \emph{eigenvector} with \emph{eigenvalue} $\lambda$ if:
    \begin{equation}\label{eq:eigenvalue}
    \mathbf{A}\mathbf{v} = \lambda \mathbf{v}
    \end{equation}
    Eigen-decomposition is useful in analyzing the behavior of linear operators—particularly in understanding how information or gradients propagate through transformations.
\end{itemize}

\subsection{Decompositions: SVD and PCA in the Context of Dimensionality Reduction}
\noindent
Matrix decompositions often reveal critical structures in data. They are invaluable for understanding how LLMs reduce the dimensionality of embeddings or layer representations.

\begin{itemize}
    \item \textbf{Singular Value Decomposition (SVD).}
    Any matrix $\mathbf{M} \in \mathbb{R}^{m \times n}$ can be decomposed as:
    \begin{equation}\label{eq:svd}
    \mathbf{M} = \mathbf{U} \boldsymbol{\Sigma} \mathbf{V}^\top
    \end{equation}
    where $\mathbf{U} \in \mathbb{R}^{m \times m}$ and $\mathbf{V} \in \mathbb{R}^{n \times n}$ are orthogonal matrices, and $\boldsymbol{\Sigma}$ is a diagonal matrix containing singular values. SVD can help analyze rank, highlight important directions in data, and sometimes compress matrices in model distillation.

    \item \textbf{Principal Component Analysis (PCA).}
    A special case of SVD applied to covariance matrices, PCA identifies the directions (principal components) of maximum variance in the data. In NLP, PCA can be used to visualize high-dimensional embeddings or reduce dimensionality, simplifying subsequent tasks or analyses.
\end{itemize}

\noindent
By mastering these foundational linear algebra concepts—vector operations, matrix multiplication, eigenvalues/eigenvectors, and decompositions—readers will be well-prepared to grasp more advanced methods used throughout large-scale neural network architectures. In the chapters ahead, we will apply these fundamentals to topics such as attention calculations, optimization techniques, and representational analyses of LLMs. 
