\section{Positional Encodings}
\label{sec:positional_encodings}
% PROMPT: Compare sinusoidal to learned positional embeddings

\noindent
Transformers dispense with the notion of sequence order imposed by recurrent or convolutional architectures, requiring an explicit way to inject positional information into token embeddings. \textbf{Positional encodings} achieve this by adding (or concatenating) position-dependent vectors to the input embeddings, enabling the model to distinguish the order of tokens. Below, we delve into the common \textbf{sinusoidal} approach and briefly discuss some alternative methods.

\subsection{Sinusoidal Positional Embeddings: The Math Behind Them}
\noindent
In the original Transformer architecture, each position $pos$ in the input sequence is mapped to a \emph{sinusoidal} embedding vector of dimension $d_\text{model}$. For each dimension $i \in \{0, \ldots, d_\text{model}-1\}$, the encoding is defined as:
\[
\begin{aligned}
\text{PE}(pos, 2i) &= \sin\Bigl(pos \cdot 10000^{-\frac{2i}{d_\text{model}}}\Bigr), \\[6pt]
\text{PE}(pos, 2i+1) &= \cos\Bigl(pos \cdot 10000^{-\frac{2i}{d_\text{model}}}\Bigr).
\end{aligned}
\]
\begin{itemize}
    \item \textbf{Variable Frequency.} The factor $10000^{-\frac{2i}{d_\text{model}}}$ sets different frequencies for each dimension. Lower dimensions vary more slowly with respect to $pos$, while higher dimensions vary more quickly.
    \item \textbf{Relative Distance.} The combination of sines and cosines at varying frequencies allows the model to easily compute relative distances between positions, since shifting $pos$ by a fixed amount produces predictable phase shifts in these trigonometric functions.
    \item \textbf{Implementation.} During training, these positional vectors are added to (or concatenated with) the token embeddings before being passed to the first Transformer layer.
\end{itemize}

\noindent
\textbf{Why Sines and Cosines?} Beyond their periodic nature, sine and cosine functions ensure that positions far outside the training range still produce valid embeddings—unlike learned positional vectors that might not generalize to sequence lengths unseen during training.

\subsection{Alternative Positional Encoding Methods}
\noindent
While sinusoidal encodings are conceptually elegant, multiple alternatives have emerged to address various use cases:

\textbf{Learned Positional Embeddings}\index{positional embeddings!learned} assign a trainable vector to each possible position index. This approach can sometimes improve performance on in-distribution sequence lengths but may generalize poorly to longer sequences than seen in training.

\textbf{Relative Position Representations}\index{positional embeddings!relative} focus on how tokens relate to each other's positions rather than absolute indices. This approach often leads to better handling of long sequences and repetitive patterns, particularly in models like BERT or T5.

\textbf{Rotary Positional Embeddings (RoPE)}\index{positional embeddings!rotary} rotate query and key vectors in a complex plane according to their positions, potentially improving how attention scales with longer contexts. This method has gained traction in some large language models for better long-sequence extrapolation.

\textbf{Mixtures of Encodings}\index{positional embeddings!hybrid} combine different forms of positional encoding—e.g., adding a learned component on top of a sinusoidal base—to strike a balance between generalization and flexibility.

\noindent
No single encoding method universally dominates; choice often depends on the target domain (e.g., natural language vs. code), desired sequence lengths, and computational constraints. Regardless of the specific approach, \textbf{positional encodings} remain a critical design component, ensuring that Transformers can accurately model the order and structure of sequential data.
