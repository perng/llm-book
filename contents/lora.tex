% Mathematical formulation of LoRA
\begin{itemize}
    \item Traditional weight updates vs. LoRA's low-rank decomposition
    \item Matrix factorization approach: 
    \begin{equation}\label{eq:lora_decomp}
    W + BA \text{ where } B \in \mathbb{R}^{d \times r}, A \in \mathbb{R}^{r \times k}
    \end{equation}
    \item Rank $r$ as a hyperparameter controlling capacity vs. efficiency trade-off
\end{itemize}

% Implementation details
\begin{pythoncode}[LoRA Implementation]
import torch
import torch.nn as nn
import math

class LoRALayer:
    def __init__(self, base_layer, rank=4, alpha=1.0):
        self.base_layer = base_layer
        self.rank = rank
        self.alpha = alpha
        
        # Initialize A and B matrices
        self.lora_A = nn.Parameter(torch.zeros(rank, base_layer.weight.size(1)))
        self.lora_B = nn.Parameter(torch.zeros(base_layer.weight.size(0), rank))
        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))
        nn.init.zeros_(self.lora_B)
        
    def forward(self, x):
        # Regular forward pass
        base_output = self.base_layer(x)
        
        # LoRA adjustment
        lora_output = (self.lora_B @ self.lora_A @ x.T).T * (self.alpha / self.rank)
        
        return base_output + lora_output
\end{pythoncode}

% Advantages and practical considerations
\begin{itemize}
    \item Memory efficiency: Only training 
    \begin{equation}\label{eq:lora_params}
    r(d + k) \text{ parameters instead of } dk
    \end{equation}
    \item Faster training and inference compared to full fine-tuning
    \item Ability to merge LoRA weights with base model for deployment
\end{itemize}

% Mathematical analysis
\begin{itemize}
    \item Theoretical foundations of low-rank updates
    \item Impact on model capacity and expressiveness
    \item Relationship to other parameter-efficient methods
\end{itemize}

% Empirical results and best practices
\begin{itemize}
    \item Typical rank values for different tasks
    \item Guidelines for setting the scaling factor $\alpha$
    \item Performance comparisons with full fine-tuning
\end{itemize} 