\section{Review of Supervised and Unsupervised Learning}\index{supervised learning}\index{unsupervised learning}
\label{sec:review_of_supervised_and_unsupervised_learning}

\subsection{Supervised Learning}\index{supervised learning!definition}

\noindent
Supervised learning\index{supervised learning!characteristics} is a paradigm where models learn from labeled data pairs $(x, y)$\index{labeled data}, where $x$ represents the input features\index{input features} and $y$ represents the target output\index{target output}. The goal is to learn a function $f$\index{function!mapping} that maps inputs to outputs: $f: X \rightarrow Y$\index{input-output mapping}.

\subsubsection{Key Characteristics}\index{supervised learning!characteristics}
\begin{itemize}[noitemsep]
    \item Requires labeled training data\index{training data!labeled}
    \item Has clear evaluation metrics\index{evaluation metrics}
    \item Learns explicit input-output mappings\index{input-output mappings}
\end{itemize}

\subsubsection{Common Applications}\index{supervised learning!applications}
\begin{itemize}[noitemsep]
    \item Classification\index{classification} (e.g., spam detection\index{spam detection}, image recognition\index{image recognition})
    \item Regression\index{regression} (e.g., price prediction\index{price prediction}, temperature forecasting\index{temperature forecasting})
    \item Sequence labeling\index{sequence labeling} (e.g., part-of-speech tagging\index{part-of-speech tagging})
\end{itemize}

\subsubsection{Loss Functions}\index{loss functions}
\noindent
In supervised learning, loss functions\index{loss functions!definition} quantify how well our model's predictions match the true values. Different tasks require different loss functions:

\paragraph{Mean Squared Error (MSE)}\index{MSE|see {Mean Squared Error}}\index{Mean Squared Error}
Commonly used for regression tasks\index{regression!tasks}, MSE measures the average squared difference between predictions and true values:

\[ MSE = \frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2 \]

The following code demonstrates the implementation of Mean Squared Error loss function:

\begin{pythoncode}[Mean Squared Error Implementation]
import numpy as np

def mse_loss(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)

# Example usage
y_true = np.array([1.0, 2.0, 3.0])
y_pred = np.array([1.1, 2.2, 2.8])
loss = mse_loss(y_true, y_pred)  # Output: 0.05
\end{pythoncode}

\paragraph{Cross-Entropy Loss}\index{cross-entropy loss}
Used primarily for classification tasks\index{classification!tasks}, cross-entropy loss measures the difference between predicted probability distributions\index{probability distributions} and true labels\index{true labels}:

\[ H(y, \hat{y}) = -\sum_{i=1}^n y_i \log(\hat{y}_i) \]

The implementation of cross-entropy loss with numerical stability considerations is shown in the following code:

\begin{pythoncode}[Cross-Entropy Loss Implementation]
def cross_entropy_loss(y_true, y_pred):
    # Add small epsilon to avoid log(0)
    epsilon = 1e-15
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    return -np.sum(y_true * np.log(y_pred))

# Example for binary classification
y_true = np.array([1, 0, 1])
y_pred = np.array([0.9, 0.1, 0.8])
loss = cross_entropy_loss(y_true, y_pred)
\end{pythoncode}

\subsubsection{Optimization Algorithms}\index{optimization algorithms}
The process of minimizing loss functions typically involves gradient descent\index{gradient descent} and its variants.

\paragraph{Stochastic Gradient Descent (SGD)}\index{SGD|see {Stochastic Gradient Descent}}\index{Stochastic Gradient Descent}
Basic SGD updates parameters using the gradient of the loss with respect to each parameter:

\[ \theta_{t+1} = \theta_t - \eta \nabla_\theta J(\theta) \]

A basic implementation of SGD is shown in the following code:

\begin{pythoncode}[Stochastic Gradient Descent Implementation]
import numpy as np
def sgd_update(params, grads, learning_rate=0.01):
    return params - learning_rate * grads

# Example usage
weights = np.array([0.5, -0.2, 0.1])
gradients = np.array([0.1, -0.05, 0.02])
weights = sgd_update(weights, gradients)
\end{pythoncode}

\paragraph{SGD with Momentum}\index{momentum}\index{SGD!with momentum}
Momentum helps accelerate SGD by accumulating a velocity vector\index{velocity vector} in directions of persistent reduction in the objective. The following code demonstrates the implementation of SGD with momentum.

\begin{pythoncode}[SGD with Momentum Implementation]
import numpy as np
def sgd_momentum_update(params, velocity, grads, 
                        learning_rate=0.01, momentum=0.9):
    velocity = momentum * velocity - learning_rate * grads
    return params + velocity, velocity

# Example usage
weights = np.array([0.5, -0.2, 0.1])
velocity = np.zeros_like(weights)
gradients = np.array([0.1, -0.05, 0.02])
weights, velocity = sgd_momentum_update(weights, velocity, gradients)
\end{pythoncode}

\paragraph{Adam Optimizer}\index{Adam optimizer}
Adam combines ideas from momentum\index{momentum} and RMSprop\index{RMSprop}, maintaining per-parameter learning rates\index{learning rates}. The implementation with bias correction is shown in the following code:

\begin{pythoncode}[Adam Optimizer Implementation]
import numpy as np
def adam_update(params, m, v, grads, t, 
                learning_rate=0.001, beta1=0.9, beta2=0.999):
    epsilon = 1e-8
    
    # Update biased first moment estimate
    m = beta1 * m + (1 - beta1) * grads
    # Update biased second moment estimate
    v = beta2 * v + (1 - beta2) * grads**2
    
    # Bias correction
    m_hat = m / (1 - beta1**t)
    v_hat = v / (1 - beta2**t)
    
    # Update parameters
    params = params - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)
    return params, m, v

# Example usage
weights = np.array([0.5, -0.2, 0.1])
m = np.zeros_like(weights)
v = np.zeros_like(weights)
gradients = np.array([0.1, -0.05, 0.02])
t = 1  # timestep
weights, m, v = adam_update(weights, m, v, gradients, t)
\end{pythoncode}

\subsection{Practical Considerations}\index{practical considerations}

\noindent
When implementing these optimization algorithms\index{optimization!implementation}, several factors should be considered:

\begin{itemize}[noitemsep]
    \item Learning rate selection\index{learning rate!selection} is crucial and often requires tuning\index{hyperparameter tuning}
    \item Batch size\index{batch size} affects both training stability\index{training stability} and computational efficiency\index{computational efficiency}
    \item Initialization of parameters\index{parameter initialization} can significantly impact convergence\index{convergence}
    \item Regularization techniques\index{regularization} help prevent overfitting\index{overfitting}
\end{itemize}

\subsection{Unsupervised Learning}\index{unsupervised learning!definition}

\noindent
Unsupervised learning involves finding patterns or structure in unlabeled data\index{unlabeled data}. Unlike supervised learning, there are no explicit target outputs to learn from. Instead, these algorithms must discover inherent patterns, relationships, and structures within the data itself. This approach is particularly valuable in scenarios where labeling data is expensive, impractical, or impossible.

\subsubsection{Key Characteristics}\index{unsupervised learning!characteristics}
\begin{itemize}[noitemsep]
    \item Works with unlabeled data\index{data!unlabeled}
    \item Focuses on finding patterns and structure\index{pattern finding}
    \item Often more exploratory in nature\index{exploratory analysis}
    \item Requires different evaluation approaches
    \item Results may be more subjective to interpret
    \item Can reveal unexpected patterns in data
\end{itemize}

\subsubsection{Common Applications}\index{unsupervised learning!applications}
\begin{itemize}[noitemsep]
    \item \textbf{Clustering}\index{clustering}
    Clustering algorithms group similar data points together, revealing natural categories or segments within the data. This is particularly useful in customer segmentation\index{customer segmentation}, where businesses can identify distinct customer groups based on behavior patterns, or in document clustering\index{document clustering}, where similar texts are grouped together.
    
    \item \textbf{Dimensionality Reduction}\index{dimensionality reduction}
    These techniques transform high-dimensional data into lower-dimensional representations while preserving important information. Methods like Principal Component Analysis (PCA)\index{PCA} and t-SNE\index{t-SNE} are widely used for data visualization and preprocessing. In language processing, this helps create more manageable representations of text data.
    
    \item \textbf{Anomaly Detection}\index{anomaly detection}
    By learning the normal patterns in data, these algorithms can identify unusual or suspicious instances. This is crucial in fraud detection\index{fraud detection}, system health monitoring\index{system monitoring}, and quality control applications.
    
    \item \textbf{Feature Learning}\index{feature learning}
    Unsupervised methods can automatically discover useful features or representations from raw data. This is particularly important in deep learning, where autoencoders\index{autoencoders} can learn compressed representations of input data.
\end{itemize}

\subsubsection{Relevance to Language Models}\index{language models!unsupervised aspects}
\noindent
Unsupervised learning plays a fundamental role in modern language models in several ways:

\begin{itemize}[noitemsep]
    \item \textbf{Word Embeddings}\index{word embeddings}
    Word embedding techniques learn to represent words as dense vectors by analyzing their usage patterns in large text corpora. These representations capture semantic relationships between words without requiring explicit labels.
    
    \item \textbf{Topic Modeling}\index{topic modeling}
    These algorithms discover abstract topics that occur in collections of documents, helping to organize and summarize large text collections automatically.
    
    \item \textbf{Self-Supervised Learning}\index{self-supervised learning}
    Modern language models use self-supervision, where the supervision signal is automatically derived from the input data itself. This approach bridges supervised and unsupervised learning, allowing models to learn from vast amounts of unlabeled text.
\end{itemize}

\subsubsection{Challenges and Considerations}\index{unsupervised learning!challenges}
\noindent
While powerful, unsupervised learning faces several important challenges:

\begin{itemize}[noitemsep]
    \item \textbf{Evaluation Difficulty}\index{evaluation difficulty}
    Without ground truth labels, it can be challenging to quantitatively assess the quality of results.
    
    \item \textbf{Interpretation Complexity}\index{interpretation complexity}
    The patterns discovered may be difficult to interpret or validate, requiring domain expertise.
    
    \item \textbf{Parameter Selection}\index{parameter selection}
    Many algorithms require careful selection of parameters (like the number of clusters), which can significantly impact results.
    
    \item \textbf{Scalability}\index{scalability}
    Some techniques become computationally expensive with large datasets, requiring efficient implementations or approximations.
\end{itemize}

\subsubsection{Future Directions}\index{unsupervised learning!future directions}
\noindent
The field continues to evolve with several promising directions:

\begin{itemize}[noitemsep]
    \item Integration with deep learning architectures
    \item Development of more interpretable models
    \item Improved evaluation metrics and validation techniques
    \item Enhanced scalability for large-scale applications
    \item Novel applications in language understanding and generation
\end{itemize}

\subsection{The Bridge to Language Models}\index{language models!connection to learning paradigms}

Understanding these fundamental learning paradigms is crucial for grasping how Large Language Models work\index{LLM!learning paradigms}, as they incorporate aspects of both:

\begin{itemize}[noitemsep]
    \item Like supervised learning\index{supervised learning!in LLMs}, LLMs learn patterns from input-output pairs during pre-training\index{pre-training} (e.g., predicting the next word)
    \item Like unsupervised learning\index{unsupervised learning!in LLMs}, they discover latent patterns\index{latent patterns} and structures in language without explicit labeling
    \item They introduce new concepts like self-supervision\index{self-supervision}, where the supervision signal is automatically derived from the input data itself
\end{itemize}

\subsection{Key Differences in Scale and Approach}

Traditional supervised and unsupervised learning typically operate on:
\begin{itemize}[noitemsep]
    \item Smaller, carefully curated datasets
    \item Well-defined problem spaces
    \item Specific tasks with clear evaluation metrics
\end{itemize}

In contrast, modern LLMs:
\begin{itemize}[noitemsep]
    \item Use massive datasets (billions of tokens)
    \item Learn general-purpose representations
    \item Can adapt to multiple tasks without task-specific training
\end{itemize}

This foundation in classical machine learning concepts helps us better understand both the innovations and limitations of modern language models, which we'll explore in subsequent chapters. 