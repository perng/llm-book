@article{vaswani2017attention,
    title={Attention is all you need},
    author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
    journal={Advances in neural information processing systems},
    volume={30},
    year={2017}
}

@article{devlin2018bert,
    title={Bert: Pre-training of deep bidirectional transformers for language understanding},
    author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
    journal={arXiv preprint arXiv:1810.04805},
    year={2018}
}

@article{brown2020language,
    title={Language models are few-shot learners},
    author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
    journal={Advances in neural information processing systems},
    volume={33},
    pages={1877--1901},
    year={2020}
}

@article{hochreiter1997long,
    title={Long short-term memory},
    author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
    journal={Neural computation},
    volume={9},
    number={8},
    pages={1735--1780},
    year={1997},
    publisher={MIT Press}
}

@article{bahdanau2014neural,
    title={Neural machine translation by jointly learning to align and translate},
    author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
    journal={arXiv preprint arXiv:1409.0473},
    year={2014}
}

@article{kaplan2020scaling,
    title={Scaling laws for neural language models},
    author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
    journal={arXiv preprint arXiv:2001.08361},
    year={2020}
}

@article{radford2019language,
    title={Language models are unsupervised multitask learners},
    author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
    journal={OpenAI blog},
    volume={1},
    number={8},
    pages={9},
    year={2019}
}

@article{liu2019roberta,
    title={Roberta: A robustly optimized bert pretraining approach},
    author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
    journal={arXiv preprint arXiv:1907.11692},
    year={2019}
}

@article{raffel2020exploring,
    title={Exploring the limits of transfer learning with a unified text-to-text transformer},
    author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
    journal={Journal of Machine Learning Research},
    volume={21},
    pages={1--67},
    year={2020}
}

@article{chowdhery2022palm,
    title={PaLM: Scaling language modeling with pathways},
    author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
    journal={arXiv preprint arXiv:2204.02311},
    year={2022}
}

@article{hoffmann2022training,
    title={Training compute-optimal large language models},
    author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
    journal={arXiv preprint arXiv:2203.15556},
    year={2022}
}

@article{touvron2023llama,
    title={LLaMA: Open and efficient foundation language models},
    author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
    journal={arXiv preprint arXiv:2302.13971},
    year={2023}
}

@article{zhang2023llama2,
    title={Llama 2: Open foundation and fine-tuned chat models},
    author={Zhang, Yunfan and Sun, Jisheng and Zhang, Yixin and Zhang, Ming and others},
    journal={arXiv preprint arXiv:2307.09288},
    year={2023}
}

@article{anthropic2023claude,
    title={Constitutional AI: A Framework for Machine Learning Systems that Interact with Humans},
    author={Anthropic},
    journal={arXiv preprint arXiv:2310.07590},
    year={2023}
}

@article{wei2022emergent,
    title={Emergent abilities of large language models},
    author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
    journal={Transactions on Machine Learning Research},
    year={2022}
}

@article{hu2021lora,
    title={LoRA: Low-rank adaptation of large language models},
    author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Chen, Weizhu},
    journal={arXiv preprint arXiv:2106.09685},
    year={2021}
}

@article{ouyang2022training,
    title={Training language models to follow instructions with human feedback},
    author={Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
    journal={Advances in Neural Information Processing Systems},
    volume={35},
    pages={27730--27744},
    year={2022}
}

@article{clark2022unified,
    title={A unified view of likelihood and attention in neural machine translation},
    author={Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D},
    journal={arXiv preprint arXiv:2108.12672},
    year={2022}
}

@article{fedus2021switch,
    title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
    author={Fedus, William and Zoph, Barret and Shazeer, Noam},
    journal={arXiv preprint arXiv:2101.03961},
    year={2021}
}

@article{wang2019superglue,
    title={SuperGLUE: A stickier benchmark for general-purpose language understanding systems},
    author={Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
    journal={arXiv preprint arXiv:1905.00537},
    year={2019}
}

@article{dettmers2023qlora,
    title={QLoRA: Efficient finetuning of quantized LLMs},
    author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
    journal={arXiv preprint arXiv:2305.14314},
    year={2023}
}

@article{dao2022flashattention,
    title={FlashAttention: Fast and memory-efficient exact attention with IO-awareness},
    author={Dao, Tri and Fu, Daniel Y and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
    journal={Advances in Neural Information Processing Systems},
    volume={35},
    pages={16344--16359},
    year={2022}
}

@article{zhang2022opt,
    title={OPT: Open pre-trained transformer language models},
    author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
    journal={arXiv preprint arXiv:2205.01068},
    year={2022}
}

@article{black2022gpt,
    title={GPT-NeoX-20B: An open-source autoregressive language model},
    author={Black, Sid and Gao, Leo and Wang, Phil and Leahy, Connor and Biderman, Stella},
    journal={arXiv preprint arXiv:2204.06745},
    year={2022}
}

@article{anil2023palm,
    title={PaLM 2 technical report},
    author={Anil, Rohan and Dai, Andrew M and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Peter and Chen, Zhifeng and others},
    journal={arXiv preprint arXiv:2305.10403},
    year={2023}
}

@article{rae2021scaling,
    title={Scaling language models: Methods, analysis \& insights from training Gopher},
    author={Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},
    journal={arXiv preprint arXiv:2112.11446},
    year={2021}
}

@article{thoppilan2022lamda,
    title={LaMDA: Language models for dialog applications},
    author={Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and others},
    journal={arXiv preprint arXiv:2201.08239},
    year={2022}
}

@article{rumelhart1986learning,
    title={Learning representations by back-propagating errors},
    author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
    journal={Nature},
    volume={323},
    number={6088},
    pages={533--536},
    year={1986},
    publisher={Nature Publishing Group}
}

@article{elman1990finding,
    title={Finding structure in time},
    author={Elman, Jeffrey L},
    journal={Cognitive science},
    volume={14},
    number={2},
    pages={179--211},
    year={1990},
    publisher={Wiley Online Library}
}

@inproceedings{cho2014learning,
    title={Learning phrase representations using RNN encoder-decoder for statistical machine translation},
    author={Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
    booktitle={Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
    pages={1724--1734},
    year={2014}
}

@article{graves2013generating,
    title={Generating sequences with recurrent neural networks},
    author={Graves, Alex},
    journal={arXiv preprint arXiv:1308.0850},
    year={2013}
}

@article{pascanu2013difficulty,
    title={On the difficulty of training recurrent neural networks},
    author={Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
    journal={International conference on machine learning},
    pages={1310--1318},
    year={2013},
    publisher={PMLR}
}

@article{radford2018improving,
    title={Improving language understanding by generative pre-training},
    author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
    journal={OpenAI Blog},
    year={2018}
}

@article{yang2019xlnet,
    title={Xlnet: Generalized autoregressive pretraining for language understanding},
    author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ and Le, Quoc V},
    journal={Advances in neural information processing systems},
    volume={32},
    year={2019}
}

@article{wang2018glue,
    title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
    author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
    journal={arXiv preprint arXiv:1804.07461},
    year={2018}
}

@article{rajpurkar2016squad,
    title={SQuAD: 100,000+ Questions for Machine Comprehension of Text},
    author={Rajpurkar, Pranav and Zhang, Junjie and Lopyrev, Konstantin and Liang, Percy},
    journal={Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
    pages={2383--2392},
    year={2016}
}

@article{srivastava2022beyond,
    title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
    author={Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeybi, Mohammad and Patwary, Mostofa and Korthikanti, Vishnu and Ozdayi, Elham and Guan, Yifan and others},
    journal={arXiv preprint arXiv:2206.04615},
    year={2022}
}

@article{mikolov2013efficient,
    title={Efficient estimation of word representations in vector space},
    author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
    journal={arXiv preprint arXiv:1301.3781},
    year={2013}
}

@article{pennington2014glove,
    title={GloVe: Global vectors for word representation},
    author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
    journal={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
    pages={1532--1543},
    year={2014}
}

@article{bojanowski2017enriching,
    title={Enriching word vectors with subword information},
    author={Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
    journal={Transactions of the Association for Computational Linguistics},
    volume={5},
    pages={135--146},
    year={2017}
}

@inproceedings{grover2016node2vec,
    title={node2vec: Scalable feature learning for networks},
    author={Grover, Aditya and Leskovec, Jure},
    booktitle={Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining},
    pages={855--864},
    year={2016}
}

@inproceedings{perozzi2014deepwalk,
    title={DeepWalk: Online learning of social representations},
    author={Perozzi, Bryan and Al-Rfou, Rami and Skiena, Steven},
    booktitle={Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining},
    pages={701--710},
    year={2014}
}

@article{hendrycks2020measuring,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Burns, Andy and Yuan, Li and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}


@article{liang2022holistic,
  title={Holistic evaluation of language models},
  author={Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Apoorv and Wu, Yuhuai and Kumar, Ananya and others},
  journal={arXiv preprint arXiv:2211.09110},
  year={2022}
}

@inproceedings{rajpurkar2018know,
  title={Know what you donâ€™t know: Unanswerable questions for SQuAD},
  author={Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={784--789},
  year={2018}
}

@inproceedings{kwiatkowski2019natural,
  title={Natural questions: a benchmark for question answering research},
  author={Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others},
  booktitle={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={452--466},
  year={2019}
}

@inproceedings{joshi2017triviaqa,
  title={Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension},
  author={Joshi, Mandar and Choi, Eunsol and Weld, Daniel S and Zettlemoyer, Luke},
  booktitle={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1601--1611},
  year={2017}
}

@inproceedings{yang2018hotpotqa,
  title={Hotpotqa: A dataset for diverse, explainable multi-hop question answering},
  author={Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William and Salakhutdinov, Ruslan and Manning, Christopher D},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={2369--2380},
  year={2018}
}

@inproceedings{zellers2019hellaswag,
  title={Hellaswag: Can a machine really finish your sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={4791--4800},
  year={2019}
}

@inproceedings{clark2018think,
    title = "{Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge}",
    author = {Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
    booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
    year = {2018},
    pages = {2347--2356},
    location = {Brussels, Belgium},
    publisher = {Association for Computational Linguistics}
}


@article{liu2020logiqa,
  title={LogiQA: A challenge dataset for machine reading comprehension with logical reasoning},
  author={Liu, Jian and Chen, Leyang and Zhao, Hanmeng and Zhu, Xijun and Zhao, Zhuoyu and Zhang, Bingjie and Li, Ruixin},
  journal={arXiv preprint arXiv:2007.08186},
  year={2020}
}

@inproceedings{lin2020birds,
  title={Birds have four legs?! NumerSense: Probing numerical commonsense knowledge of pre-trained language models},
  author={Lin, Bill Yuchen and Tafjord, Oyvind and Clark, Peter and Ren, Xiang},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={6977--6983},
  year={2020}
}

@article{hendrycks2021measuring,
  title={Measuring mathematical problem solving with the math dataset},
  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akshika and Basart, Steven and Puranik, Eric and Horowit, Tomer and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2103.03874},
  year={2021}
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique P d O and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{austin2021program,
  title={Program synthesis with large language models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}

@inproceedings{hermann2015teaching,
  title={Teaching machines to read and comprehend},
  author={Hermann, Karl Moritz and Kocisky, Tomas and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
  booktitle={Advances in neural information processing systems},
  pages={1693--1701},
  year={2015}
}

@inproceedings{narayan2018don,
  title={Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization},
  author={Narayan, Shashi and Cohen, Shay B and Lapata, Mirella},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={1797--1807},
  year={2018}
}

@inproceedings{zhang2018personalizing,
  title={Personalizing dialogue agents: I have my preferences, and you have yours},
  author={Zhang, Saizheng and Dinan, Emily and Urbanek, Jack and Szlam, Arthur and Kiela, Douwe and Weston, Jason},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2204--2213},
  year={2018}
}

@inproceedings{budzianowski2018multiwoz,
  title={Multiwoz: A large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling},
  author={Budzianowski, Pawe{\l} and Wen, Tsung-Hsien and Tseng, Bo-Hsiang and Casanueva, I{\~n}igo and Ultes, Stefan and Ramadan, Osman and Ga{\v{s}}i{\'c}, Milica},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={5016--5026},
  year={2018}
}

@article{lin2021truthfulqa,
  title={Truthfulqa: Measuring how models mimic human falsehoods},
  author={Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  journal={arXiv preprint arXiv:2109.07958},
  year={2021}
}

@article{hartvigsen2022toxigen,
  title={Toxigen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection},
  author={Hartvigsen, Thomas and Gabriel, Saadia and Palangi, Hamid and Sap, Maarten and Ray, Dipankar},
  journal={arXiv preprint arXiv:2203.09509},
  year={2022}
}

@article{gehman2020realtoxicityprompts,
  title={Realtoxicityprompts: Evaluating neural toxic degeneration in language models},
  author={Gehman, Samuel and Gururangan, Suchin and Sap, Maarten and Choi, Yejin and Smith, Noah A},
  journal={arXiv preprint arXiv:2009.11462},
  year={2020}
}

@inproceedings{clark2019boolq,
  title={BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions},
  author={Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={2924--2936},
  year={2019}
}

@inproceedings{de2019commitmentbank,
  title={The commitmentbank: A corpus of naturally occurring commitments},
  author={De Marneffe, Marie-Catherine and Roessner, Mandy and Bhatia, Archna and Carlson, Sam and Chen, Junwen and Yu, Song Feng and White, Aaron Steven and Simons, Judith},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={5203--5213},
  year={2019}
}

@inproceedings{roemmele2011choice,
  title={Choice of plausible alternatives: An evaluation of commonsense causal reasoning},
  author={Roemmele, Melissa and Bejan, Cosmin and Gordon, Andrew S},
  booktitle={2011 AAAI Spring Symposium Series},
  year={2011}
}

@inproceedings{khashabi2018looking,
  title={Looking beyond the surface: A challenge set for reading comprehension over multiple sentences},
  author={Khashabi, Daniel and Min, Seungwhan and Khot, Tushar and Sabharwal, Ashish and Tafjord, Oyvind and Clark, Peter and Hajishirzi, Hannaneh},
  booktitle={Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
  pages={252--262},
  year={2018}
}

@article{zhang2018record,
  title={Record: Bridging the gap between human and machine commonsense reading comprehension},
  author={Zhang, Sheng and Liu, Xiaodong and Chen, Jing and Gao, Jianfeng and Chen, Lijuan and Zhao, Dapeng and Zhao, Haijun},
  journal={arXiv preprint arXiv:1810.12885},
  year={2018}
}

@inproceedings{pilehvar2018wic,
  title={Wic: the word-in-context dataset for evaluation of context-sensitive meaning representations},
  author={Pilehvar, Mohammad Taher and Camacho-Collados, Jose},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={1267--1273},
  year={2018}
}

@inproceedings{levesque2012winograd,
  title={The winograd schema challenge},
  author={Levesque, Hector J and Davis, Ernest and Morgenstern, Leora},
  booktitle={Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning},
  year={2012}
} 

@article{bengio2003neural,
  title={A neural probabilistic language model},
  author={Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Jauvin, Christian},
  journal={Journal of Machine Learning Research},
  volume={3},
  number={Feb},
  pages={1137--1155},
  year={2003}
}