\documentclass[12pt]{book}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{amsmath,amssymb,amsthm}
\usepackage[colorlinks=true,
            linkcolor=blue,
            filecolor=magenta,
            urlcolor=cyan,
            citecolor=green,
            bookmarks=true,
            bookmarksopen=true]{hyperref}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{minted}
\usepackage{enumitem}
\setlist{noitemsep}

\usepackage{tcolorbox}
\tcbuselibrary{listings,minted,skins}

\newtcblisting{pythoncode}[1][Python Implementation]{
  listing engine=minted,
  colback=blue!5!white,
  colframe=blue!75!black,
  listing only,
  minted language=python,
  minted options={fontsize=\small,breaklines,autogobble,linenos,numbersep=3mm},
  left=5mm,
  enhanced,
  title=#1,
  arc=0.3mm,
  boxrule=0.8pt
}

\let\oldtableofcontents\tableofcontents
\renewcommand{\tableofcontents}{\oldtableofcontents\clearpage}

\title{A Mathematical Introduction to Large Language Models}
\author{Author Name}
\date{\today}

\begin{document}

\maketitle

\tableofcontents

\part{Foundations}

\chapter{Introduction to Large Language Models}
\noindent
Large Language Models (LLMs) have fundamentally transformed Natural Language Processing (NLP) by scaling up model capacity and data to unprecedented levels. In this opening chapter, we set the stage for how LLMs emerged from simpler statistical methods, highlight their defining characteristics, and provide an overview of the key mathematical and architectural concepts that will be explored throughout the book.

\input{contents/historical.tex}
\input{contents/what_makes_llms_different.tex}
\input{contents/book_organization.tex}

\chapter{Essential Mathematics}
\noindent
A solid grasp of linear algebra, calculus, and probability is vital for understanding how large language models learn from vast textual data. In this chapter, we review the core mathematical principles that underpin neural network operations, optimization procedures, and the statistical modeling approaches integral to LLMs. By establishing these fundamentals, readers will be well-equipped to dive into more specialized topics in subsequent chapters.

\input{contents/linear_algebra.tex}
\input{contents/calculus.tex}
\input{contents/probability.tex}


\chapter{Machine Learning Fundamentals}
\noindent
Before delving into the specific architecture of LLMs, it is crucial to have a firm grounding in basic machine learning concepts. This chapter provides a concise overview of essential principles—such as loss functions, gradient-based optimization, and regularization—that form the backbone of modern neural network training. By exploring these fundamentals, we set the stage for understanding advanced techniques used in large-scale training.

\input{contents/review_of_supervised_and_unsupervised_learning.tex}
\input{contents/neural_networks_101.tex}
\input{contents/reg_and_gen.tex}

\part{Transformer Architecture and Attention}

\chapter{The Attention Mechanism}
\noindent
The invention of attention-based methods revolutionized NLP by enabling models to capture long-range dependencies without the bottlenecks of recurrence. In this chapter, we unpack the math behind self-attention and scaled dot-product attention, illustrating why attention layers are so effective in Transformers. These insights pave the way for understanding multi-head attention and its role in capturing diverse contextual relationships.


\input{contents/self_attention.tex}
\input{contents/multi_head_attention.tex}


\chapter{Transformer Encoder-Decoder Structure}
\noindent
Building upon attention, the Transformer architecture introduces a powerful encoder-decoder framework that handles complex sequence-to-sequence tasks efficiently. This chapter explores how positional encodings, residual connections, and normalization layers come together in both encoder and decoder blocks. By the end, you will see how all these components integrate mathematically to form the backbone of modern LLMs.

\input{contents/positional_encodings.tex}
\input{contents/encoder_block.tex}

\part{Training Large Language Models}
\input{contents/lm_pretraining.tex}
\input{contents/large_scale_optimization.tex}
\input{contents/scaling_laws.tex}

\part{Advanced Topics and Future Directions}
\input{contents/fine_tuning_and_prompt_engineering.tex}
\input{contents/emergent_abilities_and_interpretability.tex}
\input{contents/efficient_serving_and_deployment.tex}
\input{contents/future_directions.tex}

\part{Conclusion and Resources}
\input{contents/conclusion.tex}

\appendix
\chapter{Mathematical Proofs and Derivations}

\chapter{Notation Guide}

\chapter{Glossary of Terms}

\end{document}
